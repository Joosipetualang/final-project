{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xV6hlFGsfZd"
      },
      "outputs": [],
      "source": [
        "#!pip install pypdf==3.17.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EN2HR-_sfZf"
      },
      "outputs": [],
      "source": [
        "\n",
        "#!pip install numpy==1.26.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJzUaUkisfZf"
      },
      "outputs": [],
      "source": [
        "#!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3zVTiuvsfZg"
      },
      "outputs": [],
      "source": [
        "#   pip install fpdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GvKMyoCsfZg"
      },
      "outputs": [],
      "source": [
        "# Provide either INPUT_FILE path or INPUT_TEXT to summarize.\n",
        "INPUT_FILE=\"examplee.pdf\" # Insert file path here\n",
        "\n",
        "\n",
        "# Style of summarization:\n",
        "\n",
        "# Numbered List style\n",
        "STYLE=\"Return your response as detailed summary which covers the main points of the text and key facts and figures.\"\n",
        "PROMPT_TRIGGER=\"DETAILED SUMMARY\"\n",
        "\n",
        "# One sentence style\n",
        "# STYLE=\"Return your response as one sentence which covers the main points of the text.\",\n",
        "# PROMPT_TRIGGER=\"ONE SENTENCE SUMMARY\",\n",
        "\n",
        "# Concise style\n",
        "# STYLE=\"Return your response as concise summary which covers the main points of the text.\",\n",
        "# PROMPT_TRIGGER=\"CONCISE SUMMARY\",\n",
        "\n",
        "# Detailed style\n",
        "# STYLE=\"Return your response as detailed summary which covers the main points of the text and key facts and figures.\",\n",
        "# PROMPT_TRIGGER=\"DETAILED SUMMARY\",\n",
        "\n",
        "# Output language, try e.g. Polish, Spanish, etc\n",
        "OUTPUT_LANGUAGE = \"Indonesia\"\n",
        "\n",
        "# Should output verbose info from underlying models, etc.\n",
        "VERBOSE=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-xOite_sfZh"
      },
      "outputs": [],
      "source": [
        "MODEL_CONTEXT_WINDOW=8192\n",
        "\n",
        "# Maximal lenght of model's output, in tokens.\n",
        "MAX_ANSWER_TOKENS = 4048\n",
        "\n",
        "# Chunk params in characters (not tokens).\n",
        "CHUNK_SIZE=10000\n",
        "CHUNK_OVERLAP=500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "sZFeNnG5sfZh",
        "outputId": "fade4f5e-6b03-45bd-9403-0a9887eef5fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6564\\666015976.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
            "  llm = Ollama(model=\"mistral-openorca\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " After Jonny gives Lisa 1 banana, he has 2 bananas left. Then Simon gives Jonny 2 bananas and 1 apple. So Jonny now has (3 - 1) + 2 + 1 = 4 bananas and 1 apple.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import Ollama\n",
        "\n",
        "llm = Ollama(model=\"mistral-openorca\")\n",
        "\n",
        "response = llm.invoke(\"jonny have 3 bananas and he give lisa 1 banana. after that simon give jonny 2 bananas and 1 apple. what is jonny have?\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNuLPaazsfZi"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "def load_content():\n",
        "    \"\"\"Loads INPUT_FILE if set, otherwise returns INPUT_TEXT\"\"\"\n",
        "\n",
        "    if INPUT_FILE:\n",
        "        if INPUT_FILE.endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(INPUT_FILE)\n",
        "            docs = loader.load()\n",
        "            print(f\"PDF: loaded {len(docs)} pages\")\n",
        "            return \"\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "        docs =  TextLoader(INPUT_FILE).load()\n",
        "        return docs[0].page_content\n",
        "\n",
        "    return INPUT_TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cg8Xg7tsfZi"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "combine_prompt_template = \"\"\"\n",
        "Write a summary of the following text delimited by tripple backquotes.\n",
        "{style}\n",
        "\n",
        "```{content}```\n",
        "\n",
        "{trigger} in {language}:\n",
        "\"\"\"\n",
        "\n",
        "map_prompt_template = \"\"\"\n",
        "Write a concise summary of the following:\n",
        "{text}\n",
        "\n",
        "CONCISE SUMMARY in {language}:\n",
        "\"\"\"\n",
        "\n",
        "def summarize_base(llm, content):\n",
        "    \"\"\"Summarize whole content at once. The content needs to fit into model's context window.\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(\n",
        "        combine_prompt_template\n",
        "    ).partial(\n",
        "        style=STYLE,\n",
        "        trigger=PROMPT_TRIGGER,\n",
        "        language=OUTPUT_LANGUAGE,\n",
        "    )\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt, verbose=VERBOSE)\n",
        "    output = chain.run(content)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def summarize_map_reduce(llm, content):\n",
        "    \"\"\"Summarize content potentially larger that model's context window using map-reduce approach.\"\"\"\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "    )\n",
        "\n",
        "    split_docs = text_splitter.create_documents([content])\n",
        "    print(f\"Map-Reduce content splits ({len(split_docs)} splits): {[len(sd.page_content) for sd in split_docs]}\")\n",
        "\n",
        "    map_prompt = PromptTemplate.from_template(\n",
        "        map_prompt_template\n",
        "    ).partial(\n",
        "        language=OUTPUT_LANGUAGE,\n",
        "    )\n",
        "\n",
        "    combine_prompt = PromptTemplate.from_template(\n",
        "        combine_prompt_template\n",
        "    ).partial(\n",
        "        style=STYLE,\n",
        "        trigger=PROMPT_TRIGGER,\n",
        "        language=OUTPUT_LANGUAGE,\n",
        "    )\n",
        "\n",
        "    chain = load_summarize_chain(\n",
        "        llm=llm,\n",
        "        chain_type=\"map_reduce\",\n",
        "        map_prompt=map_prompt,\n",
        "        combine_prompt=combine_prompt,\n",
        "        combine_document_variable_name=\"content\",\n",
        "        verbose=VERBOSE,\n",
        "    )\n",
        "\n",
        "    output = chain.run(split_docs)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "zTfK3AKWsfZj",
        "outputId": "932836a5-3bfa-4f00-c9b2-8baa3f961a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDF: loaded 11 pages\n",
            "Content length: 46120 chars, 12289 tokens.\n",
            "Content sample:\n",
            "Natural Language Fine-Tuning\n",
            "Jia Liu1,2,Yue Wang1,3,Zhiqi Lin1,3,Min Chen1,3∗,Yixue Hao2∗,Long Hu2∗\n",
            "1Pazhou Laboratory, Guangzhou, China\n",
            "2Huazhong University of Science and Technology, Wuhan, China\n",
            "3S\n",
            "\n",
            "\n",
            "Using summarizer: map-reduce\n",
            "Map-Reduce content splits (5 splits): [9966, 9949, 9982, 9956, 8145]\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Write a concise summary of the following:\n",
            "Natural Language Fine-Tuning\n",
            "Jia Liu1,2,Yue Wang1,3,Zhiqi Lin1,3,Min Chen1,3∗,Yixue Hao2∗,Long Hu2∗\n",
            "1Pazhou Laboratory, Guangzhou, China\n",
            "2Huazhong University of Science and Technology, Wuhan, China\n",
            "3South China University of Technology, Guangzhou, China\n",
            "{liujia0330, yixuehao, hulong }@hust.edu.cn,\n",
            "{csyuewang, 202311089192 }@mail.scut.edu.cn,\n",
            "minchen@ieee.org\n",
            "Abstract\n",
            "Large language model fine-tuning techniques typ-\n",
            "ically depend on extensive labeled data, external\n",
            "guidance, and feedback, such as human alignment,\n",
            "scalar rewards, and demonstration. However, in\n",
            "practical application, the scarcity of specific knowl-\n",
            "edge poses unprecedented challenges to existing\n",
            "fine-tuning techniques. In this paper, focusing on\n",
            "fine-tuning tasks in specific domains with limited\n",
            "data, we introduce Natural Language Fine-Tuning\n",
            "(NLFT), which utilizes natural language for fine-\n",
            "tuning for the first time. By leveraging the strong\n",
            "language comprehension capability of the target\n",
            "LM, NLFT attaches the guidance of natural lan-\n",
            "guage to the token-level outputs. Then, saliency\n",
            "tokens are identified with calculated probabilities.\n",
            "Since linguistic information is effectively utilized\n",
            "in NLFT, our proposed method significantly re-\n",
            "duces training costs. It markedly enhances training\n",
            "efficiency, comprehensively outperforming rein-\n",
            "forcement fine-tuning algorithms in accuracy, time-\n",
            "saving, and resource conservation. Additionally, on\n",
            "the macro level, NLFT can be viewed as a token-\n",
            "level fine-grained optimization of SFT, thereby effi-\n",
            "ciently replacing the SFT process without the need\n",
            "for warm-up (as opposed to ReFT requiring mul-\n",
            "tiple rounds of warm-up with SFT). Compared to\n",
            "SFT, NLFT does not increase the algorithmic com-\n",
            "plexity, maintaining O(n). Extensive experiments\n",
            "on the GSM8K dataset demonstrate that NLFT,\n",
            "with only 50 data instances, achieves an accuracy\n",
            "increase that exceeds SFT by 219%. Compared to\n",
            "ReFT, the time complexity and space complexity of\n",
            "NLFT are reduced by 78.27% and 92.24%, respec-\n",
            "tively. The superior technique of NLFT is paving\n",
            "the way for the deployment of various innovative\n",
            "LLM fine-tuning applications when resources are\n",
            "limited at network edges.\n",
            "Our code has been released at https://github.com/\n",
            "Julia-LiuJ/NLFT.\n",
            "∗Corresponding authors: Min Chen, Yixue Hao, Long Hu1 Introduction\n",
            "Figure 1: Accuracy Comparison of SFT and NLFT on GSM8K\n",
            "dataset. NLFT has the same time and space complexity as SFT but\n",
            "achieves a 27% increase in fine-tuning performance, maintaining a\n",
            "stable performance advantage thereafter. With minimal dataset sam-\n",
            "ples (only 50 data points), a brief training period (3 epochs, 287\n",
            "seconds), and low computational resource consumption (44.46 GB\n",
            "of GPU memory usage), NLFT does not require a warm-up phase\n",
            "and can achieve a performance 1.19 times greater than SFT. Accord-\n",
            "ing to the ReFT paper, ReFT is unable to outperform SFT within 8\n",
            "epochs.\n",
            "Supervised Fine-Tuning (SFT) is the most commonly\n",
            "employed method for fine-tuning Large Language Models\n",
            "(LLMs). As a foundational step in model fine-tuning, its ap-\n",
            "plication enables LLMs to better adapt to tasks across various\n",
            "domains and more effectively address specialized issues. For\n",
            "instance, in the alignment tasks, both Reinforcement Learn-\n",
            "ing with Human Feedback (RLHF) [Ouyang et al. , 2022 ]\n",
            "and Direct Preference Optimization (DPO) [Rafailov et al. ,\n",
            "2024b ]utilize SFT to provide a solid initial condition for thearXiv:2412.20382v1  [cs.CL]  29 Dec 2024\n",
            "Figure 2: Training process of SFT, ReFT, and NLFT. (a) The training process of SFT, which can be analogous to a student directly learning\n",
            "from a collection of exercises, which include problems and their reference solutions; (b) The training process of ReFT, which can be realized\n",
            "as a student repeatedly submitting their exam answers to a grading system, receiving scores, and striving to discover the strategies that\n",
            "maximize their marks; (c) The training process of our proposed algorithm NLFT, which is similar to a student submitting exam answers\n",
            "and receiving detailed feedback from a natural language evaluator. The system highlights the scoring points and losing points, allowing the\n",
            "student to learn from both well-graded examples (i.e., for learning from teaching) and their work (i.e., for self-study), thereby increasing their\n",
            "performance.\n",
            "LLM, thereby leading the learning process to be more effi-\n",
            "cient and stable. Typically, for mathematical problem solv-\n",
            "ing, researchers employ Chain-of-Thought (CoT) [Weiet al. ,\n",
            "2022 ]to annotate problems and answers, then use SFT to\n",
            "fine-tune the model for better tackling mathematical ques-\n",
            "tions [Feng et al. , 2024a ] [Chu et al. , 2023 ] [Wang et al. ,\n",
            "2022 ].\n",
            "As the application field of LLMs continues to expand, fine-\n",
            "tuning with small-scale, domain-specific data remains chal-\n",
            "lenging due to the inefficiency of existing methods in utiliz-\n",
            "ing limited samples. Consequently, researchers are progres-\n",
            "sively exploring renovated fine-tuning methods to optimize\n",
            "models. Recently, Reinforced Fine-Tuning (ReFT [Luong et\n",
            "al., 2024 ]or RFT [OpenAI, 2024 ]) has garnered widespread\n",
            "attention in the academic community for its superior perfor-\n",
            "mance in terms of accuracy increment. It employs Reinforce-\n",
            "ment Learning (RL) for the fine-tuning of model parameters,\n",
            "and thus achieving more efficient model optimization within\n",
            "the same data scale. However, introducing RL into LLMs\n",
            "results in a significant increase in time and space complex-\n",
            "ity, thereby raising the barriers to the deployment and utiliza-\n",
            "tion of this technology, especially in a mobile and/or dynamic\n",
            "network environment. Meanwhile, ReFT does not entirely re-\n",
            "place SFT. In practical applications, it still requires the SFT\n",
            "to warming-up to ensure that the model can more effectivelyadapt to specific tasks or datasets [Luong et al. , 2024 ]\n",
            "To address these issues, in this paper, we propose a novel\n",
            "minimal data fine-tuning method named Natural Language\n",
            "Fine-Tuning (NLFT). Compared to previous methods, NLFT\n",
            "utilizes natural language as the supervising signal and em-\n",
            "ploys a simple minimal data fine-tuning algorithm to enhance\n",
            "the efficiency of fine-tuning. To better illustrate the differ-\n",
            "ences among SFT, ReFT, and NLFT, we give the following\n",
            "analogy.\n",
            "LLM is analogous to a student, and LLM’s fine-tuning pro-\n",
            "cess is similar to the learning process of the student. Then,\n",
            "SFT, ReFT, and NLFT represent three individual learning\n",
            "processes of the student. Given learning math reasoning as an\n",
            "example, in SFT, the student studies in parrot-fashion, where\n",
            "the student is expected to write down a predetermined an-\n",
            "swer when seeing some particular question after screening\n",
            "numerous pairs of questions and standard answers. In ReFT,\n",
            "the student first obtains the basic technique of solving math\n",
            "reasoning problems by several epochs of SFT. Then, in or-\n",
            "der to further improve the technique, ReFT requires the stu-\n",
            "dent to submit answer sheets which include the detailed anal-\n",
            "ysis for leading the math problem solution. In ReFT, a score\n",
            "is given for each answer sheet by comparing it to the stan-\n",
            "dard answer. By the score, the student adjusts the strategy\n",
            "of math reasoning, which is similar to reinforcement learn-\n",
            "ing. Since the target of the student is to achieve a high score\n",
            "as much as possible, lots of rounds of submitting the answer\n",
            "sheet and obtaining feedback from the evaluating system are\n",
            "needed. However, in NLFT, the student’s learning process\n",
            "more closely resembles a self-study method. When the an-\n",
            "swer sheet hits majority scoring points, which represents the\n",
            "student is on a good studying track at the beginning, the stu-\n",
            "dent will re-answer the exam based on the standard answer\n",
            "and compare it with the previous one to identify the scoring\n",
            "points (see the upper part in Fig. 2c). When the answer sheet\n",
            "contains lots of incorrect points, the student will re-answer\n",
            "the exam based on both the standard answer and the judg-\n",
            "ment from external evaluating system (see the bottom part in\n",
            "Fig. 2c). After that, the losing points is identified by similar\n",
            "comparison. Repeating these steps, the student’s ability im-\n",
            "proves as the score hitting and point losing become clear. It\n",
            "should be noted that the initial ”answer sheet” may not nec-\n",
            "essarily be completed by the efforts of the student. That is if\n",
            "the student’s ability is not enough, learning from others’ an-\n",
            "swers is encouraged, especially from “good students”. If the\n",
            "student is capable, self-studying is a better strategy than the\n",
            "use of the student’s answers.\n",
            "Specifically, for the reasoning output, NLFT evaluates the\n",
            "conditional probability variations of each token under dif-\n",
            "ferent prompt conditions to allocate the saliency token. On\n",
            "this basis, the model refines the loss function based on the\n",
            "saliency level of each token, thereby enabling more efficient\n",
            "fine-tuning of the model. An overview of our algorithm is\n",
            "shown in fig.2.\n",
            "In summary, the contributions of this paper are as follows:\n",
            "1. This paper introduces NLFT, a token-level natural lan-\n",
            "guage fine-tuning algorithm. In contrast to previous\n",
            "response-level fine-tuning methods that convert natu-\n",
            "ral language into scalar rewards, our approach directly\n",
            "leverages natural language and conducts token-level an-\n",
            "notation. This method significantly improves the infor-\n",
            "mation utilization efficiency within datasets, thereby re-\n",
            "ducing the data requirements for NLFT. As a result, by\n",
            "the use of only tens to thousands of data entries, NLFT\n",
            "can achieve domain-specific fine-tuning of LLMs.\n",
            "2. In this paper, we propose a novel Minimal data fine-\n",
            "tuning that eliminates the need for a warm-up phase,\n",
            "which is required by ReFT process. Furthermore, NLFT\n",
            "achieves an accuracy rate that significantly surpasses\n",
            "SFT with a small number of data entries. Besides, due\n",
            "to the efficient algorithm design in both GPU memory\n",
            "usage and training time, our method offers a substantial\n",
            "advantage in time and memory utilization compared to\n",
            "other methods (such as ReFT).\n",
            "\n",
            "CONCISE SUMMARY in Indonesia:\n",
            "\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Write a concise summary of the following:\n",
            "2. In this paper, we propose a novel Minimal data fine-\n",
            "tuning that eliminates the need for a warm-up phase,\n",
            "which is required by ReFT process. Furthermore, NLFT\n",
            "achieves an accuracy rate that significantly surpasses\n",
            "SFT with a small number of data entries. Besides, due\n",
            "to the efficient algorithm design in both GPU memory\n",
            "usage and training time, our method offers a substantial\n",
            "advantage in time and memory utilization compared to\n",
            "other methods (such as ReFT).\n",
            "3. NLFT exhibits the extraordinary ability to solve the in-\n",
            "trinsic pitfall of overfitting phenomenon associated with\n",
            "small-sample data [Liet al. , 2019 ]. Moreover, as a\n",
            "token-level fine-tuning approach, NLFT possesses an\n",
            "outstanding interpretability compared to response-level\n",
            "fine-tuning. NLFT achieves a 64.29% accuracy with\n",
            "only 50 training data samples, exhibiting a faster im-\n",
            "provement rate compared to SFT and outperforming\n",
            "SFT by 219%, as shown in Fig. 1.2 Related Work\n",
            "2.1 Natural Language Learning\n",
            "In the past few years, much of the research in the area of LLM\n",
            "fine-tuning has focused on scalar rewards, which are less ef-\n",
            "ficient than directly using explicit natural language feedback.\n",
            "This is because scalar reward-based methods provide only an\n",
            "indirect understanding of semantic information, which can be\n",
            "suboptimal. In contrast, natural language feedback enables\n",
            "the expression with more delicate and complex preferences.\n",
            "For example, Contrastive Unlikelihood Training (CUT) [Xu\n",
            "et al. , 2023 ]uses negative judgments to align values at the to-\n",
            "ken level. Building on this idea, Natural Language Reinforce-\n",
            "ment Learning (NLRL) [Feng et al. , 2024b ]redefines RL\n",
            "principles within a natural language representation space, fur-\n",
            "ther demonstrating how natural language can facilitate both\n",
            "efficient policy optimization and improved interpretability.\n",
            "2.2 Token-level LLMs Fine-Tuning\n",
            "Response-level fine-tuning has played a significant role in\n",
            "pretraining LLMs, but it often faces challenges in terms\n",
            "of training difficulty and stability. In contrast, token-level\n",
            "fine-tuning has emerged as a promising alternative. Re-\n",
            "cently, several notable works have been proposed to im-\n",
            "prove token-level fine-tuning. For instance, [Rafailov et al. ,\n",
            "2024a ]extends DPO to a token-level Markov Decision Pro-\n",
            "cess (MDP), which enhances alignment with the autoregres-\n",
            "sive structure of LLMs and optimizes credit assignment. Ad-\n",
            "ditionally, [Zhong et al. , 2024 ]introduces Reinforced Token\n",
            "Optimization (RTO), which combines token-level rewards\n",
            "with DPO and Proximal Policy Optimization (PPO) to im-\n",
            "prove policy learning efficiency significantly. These meth-\n",
            "ods demonstrate the substantial potential of token-level fine-\n",
            "tuning in improving model performance, particularly in com-\n",
            "plex tasks where precision and consistency are crucial.\n",
            "3 Method\n",
            "In this section, we will provide a comprehensive overview\n",
            "of the token-based fine-tuning algorithm NLFT, a novel fine-\n",
            "tuning algorithm that concentrates on natural language CoT\n",
            "and its result. Assuming the input of LLM is Xand the rea-\n",
            "soning outcomes from CoT is Y={y1, y2, ..., y n}, by con-\n",
            "ducting different input prompt X, we can obtain the condi-\n",
            "tional probabilities of each token within the same CoT output\n",
            "under different input conditions. After that, by comparing\n",
            "these conditional probabilities, the saliency tokens are allo-\n",
            "cated and we perform token-level loss calculation, thereby\n",
            "achieving fine-grained tuning of the LLM. In short, the NLFT\n",
            "algorithm yields significantly superior results with time and\n",
            "space complexity compared to SFT. The detailed process of\n",
            "NLFT is shown in Algorithm 1: Natural Language Fine-\n",
            "Tuning. A more intuitive illustration can be observed in Fig.\n",
            "3.\n",
            "3.1 Preliminary Considerations\n",
            "Recently, an interesting work on AI alignment via linguis-\n",
            "tic feedback, Contrastive Unlikelihood Training(CUT) [Xu\n",
            "et al. , 2023 ]employs contrastive learning to fine-tune LLMs\n",
            "Algorithm 1 Natural Language Fine-Tuning\n",
            "1:Input: Xbase=<question> ,\n",
            "Xjudge =<question, judgment> ,\n",
            "Xstandard =< question, standard answer> , a CoT\n",
            "reasoning output Y={y1, y2, ..., y n}\n",
            "2:Output: Fine-tuned model\n",
            "3:ifYis correct then\n",
            "4: fort= 1tondo\n",
            "5: Collect P(yt|Xbase, yt−),P(yt|Xstandard , yt−)\n",
            "6: Calculate S(yt)\n",
            "7: end for\n",
            "8:else\n",
            "9: fort= 1tondo\n",
            "10: Collect P(yt|Xbase, yt−),P(yt|Xjudge, yt−),\n",
            "P(yt|Xstandard , yt−)\n",
            "11: Calculate S(yt)\n",
            "12: end for\n",
            "13:end if\n",
            "14:L=1\n",
            "NPS(yt)×logP (yt|Xbase, yt−)\n",
            "15:Fine-tune model\n",
            "16:return Fine-tuned model\n",
            "to modify erroneous output based on human negative judg-\n",
            "ments. CUT reveals that, in contrast to the error-free tokens,\n",
            "erroneous tokens experience significant variations in condi-\n",
            "tional probabilities under the two distinct input conditions of\n",
            "presence and absence of judgment. Building upon this in-\n",
            "sight, we formulate the following hypotheses through exten-\n",
            "sive experimental observations: Denoted different prompt in-\n",
            "puts as X, the CoT reasoning results as Y={y1, y2, ..., y n},\n",
            "and the conditional probability of each token under different\n",
            "input as P(yt|X, y t−), we assume that:\n",
            "1. When the output Yaligns with expectations (that\n",
            "is, the output is correct), the conditional probability\n",
            "P(yt|X, y t−)of key scoring token (Saliency Token) yt\n",
            "is significantly higher with prompt input Xstandard =<\n",
            "question, standard answer > compared to Xbase=\n",
            "< question> .\n",
            "2. When the output Yfalls to meet expectation (that\n",
            "is, the output is incorrect), the conditional proba-\n",
            "bility P(yt|X, y t−)of key scoring token (Saliency\n",
            "Token) ytexhibit substantial variations under three\n",
            "prompt inputs: Xbase =< question > ,Xjudge =\n",
            "< question, judgment > , and Xstandard =<\n",
            "question, standard answer> .\n",
            "Based on these assumptions, we introduce the NLFT algo-\n",
            "rithm. Firstly, depending on whether the output meets ex-\n",
            "pectations, the conditional probabilities of each token are ob-\n",
            "tained under different prompt inputs. After that, Saliency To-\n",
            "kens are located through contrastive learning, and the phrases\n",
            "containing these tokens are located using a semantic similar-\n",
            "ity cluster. Finally, a token-level loss function is constructed\n",
            "to achieve fine-grained fine-tuning.\n",
            "3.2 Token-level Conditional Probability Analysis\n",
            "Formally, the process of generating results through the nat-\n",
            "ural language CoT can be decomposed into a sequence of\n",
            "Figure 3: An example of the training process of NLFT, which takes\n",
            "question, standard answer, and judgment as inputs and generates dif-\n",
            "ferent Input Prompts. Then, under different prompts, the algorithm\n",
            "compares different conditional probabilities to allocate the saliency\n",
            "token and assign scale values.\n",
            "next-token prediction actions. To be specific, take the CoT\n",
            "reasoning result as Y={y1, y2, ..., y n}, where each < yt>\n",
            "is a token inferred based on the given input Xand the previ-\n",
            "ous output yt−={y1, y2, ..., y t−1}. Besides, each ythas its\n",
            "own conditional probability function P(yt|X, y t−). As pre-\n",
            "viously stated, the conditional probabilities of salient tokens\n",
            "undergo significant changes with varying inputs X. Build-\n",
            "ing on this observation, we categorize the reasoning results\n",
            "Yinto correct and incorrect outcomes, then selectively per-\n",
            "form conditional probability lookup and collection for each\n",
            "category.\n",
            "Correct Output. When the CoT reasoning output\n",
            "Yis correct, the input Xis divided into two cate-\n",
            "gories: Xbase =< question > andXstandard =<\n",
            "question, standard answer > . Then, for each token\n",
            "ytof output Y, we will have two conditional probability\n",
            "P(yt|Xbase, yt−)andP(yt|Xstandard , yt−). In following\n",
            "section, we will allocate the saliency token based on these\n",
            "conditional probabilities.\n",
            "Incorrect Output. When the reasoning output Yis incor-\n",
            "rect, the input Xis divided into three categories: Xbase=\n",
            "< question > ,Xjudge =< question, judgement > , and\n",
            "Xstandard =< question, standard answer > . Then,\n",
            "for each token ytof output Y, we will have three condi-\n",
            "tional probability P(yt|Xbase, yt−),P(yt|Xjudge, yt−), and\n",
            "P(yt|Xstandard , yt−). In the following section, we will allo-\n",
            "cate the saliency token based on these conditional probabili-\n",
            "ties.\n",
            "3.3 Probability-driven Saliency Token Allocation\n",
            "After obtaining the conditional probabilities, we proceed to\n",
            "classify them and allocate the saliency tokens. Similarly, the\n",
            "allocation strategies are divided into two categories: correct\n",
            "outcomes and incorrect outcomes.\n",
            "Correct Output. When the CoT reasoning output Yis\n",
            "correct, we set the threshold conditional probability pcorrect\n",
            "0 .\n",
            "When the conditional probability P(yt|Xstandard , yt−)>\n",
            "pcorrect\n",
            "0 , it is considered that the token is more likely to be\n",
            "adopted under Xstandard condition than the threshold condi-\n",
            "tional probability. This implies a greater correlation between\n",
            "the token and the Xstandard condition, hence it is allocated\n",
            "as a saliency token. After that, we perform semantic cluster-\n",
            "ing around these saliency tokens to identify their associated\n",
            "phrase and designate them as sub-saliency tokens. Finanlly,\n",
            "the remaining token are categorized as irrelevant tokens.\n",
            "Incorrect Output. For the incorrect CoT reasoning output\n",
            "Y, we cannot simply set a threshold conditional probability,\n",
            "as the conditional probabilities for all tokens are generally\n",
            "lower in such instances. Therefore, we calculate the follow-\n",
            "ing two ratios,\n",
            "r1=P(yt|Xjudge, yt−)\n",
            "P(yt|Xbase, yt−), (1)\n",
            "r2=P(yt|Xjudge, yt−)\n",
            "P(yt|Xstandard , yt−). (2)\n",
            "If a token is a saliency token, then its conditional probabil-\n",
            "ity under Xjudge condition should be much higher than that\n",
            "under Xbase andXstandard . Consequently, its correspond-\n",
            "ing ratios r1andr2will also be higher. Therefore, if a token\n",
            "has both its corresponding ratios r1andr2exceeding a pre-\n",
            "set value r0, and its own conditional probability surpasses the\n",
            "threshold pincorrect\n",
            "0 , the token will be allocated as a saliency\n",
            "token. After that, we also perform semantic clustering around\n",
            "these saliency tokens to identify the associated phrase. Given\n",
            "that these tokens are close to the saliency token under incor-\n",
            "\n",
            "CONCISE SUMMARY in Indonesia:\n",
            "\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Write a concise summary of the following:\n",
            "under Xbase andXstandard . Consequently, its correspond-\n",
            "ing ratios r1andr2will also be higher. Therefore, if a token\n",
            "has both its corresponding ratios r1andr2exceeding a pre-\n",
            "set value r0, and its own conditional probability surpasses the\n",
            "threshold pincorrect\n",
            "0 , the token will be allocated as a saliency\n",
            "token. After that, we also perform semantic clustering around\n",
            "these saliency tokens to identify the associated phrase. Given\n",
            "that these tokens are close to the saliency token under incor-\n",
            "rect situations, we directly categorize them as irrelevant to-\n",
            "kens and assign a scale of zero to them in subsequent con-\n",
            "trastive learning processes.\n",
            "3.4 Token-level Loss Calculation\n",
            "In this section, we will assign scale values based on the previ-\n",
            "ously allocated saliency tokens and proceed with contrastive\n",
            "learning. Similarly, the scale strategies are divided into two\n",
            "categories: correct outcomes and incorrect outcomes.\n",
            "Correct Output. As we shown before, in the correct CoT\n",
            "reasoning output, the tokens are classified into three kinds:\n",
            "saliency tokens, sub-saliency tokens, and irrelevant tokens.\n",
            "For each token, we have scales,S(yt) =\n",
            "\n",
            "1 +\u0012P(yt|Xstandard , yt−)−pcorrect\n",
            "0\n",
            "1−pcorrect\n",
            "0\u0013c1\n",
            ",\n",
            "ifyt∈Ysaliency ,\n",
            "\u0012P(yt|Xstandard , yt−)\n",
            "pcorrect\n",
            "0\u0013c2\n",
            ",\n",
            "ifyt∈Ysub−saliency ,\n",
            "\u0012P(yt|Xstandard , yt−)\n",
            "pcorrect\n",
            "0\u0013c3\n",
            ",\n",
            "ifyt∈Yirrelevant .\n",
            "(3)\n",
            "where c1,c2, and c3are hyper-parameters and c2< c 3. It\n",
            "can be observed that under such configuration, all three scales\n",
            "increase as the conditional probability P(yt|Xstandard )\n",
            "grows. Meanwhile, the scales of the saliency tokens are the\n",
            "largest and always exceed 1, while the scales of the sub-\n",
            "saliency tokens are consistently greater than that of the ir-\n",
            "relevant tokens.\n",
            "Incorrect Output. In the incorrect CoT reasoning output,\n",
            "the tokens are classified into two kinds: saliency tokens and\n",
            "irrelevant tokens. For the irrelevant tokens, we will set the\n",
            "scales to 0, as within incorrect Output, we only wish to con-\n",
            "sider the saliency tokens. For the saliency tokens, we have\n",
            "scales,\n",
            "S(yt) =2\n",
            "1 +e−(r1−r0)(4)\n",
            "The scales are larger than 0and smaller than 1, and\n",
            "these scales also increase as the conditional probability\n",
            "P(yt|Xjudge )grows.\n",
            "After obtaining the scales of each token, we get our final\n",
            "loss function,\n",
            "L=1\n",
            "N\n",
            "X\n",
            "yt∈YcorrectS(yt)×logP(yt|Xbase, yt−)\n",
            "+X\n",
            "yt∈YincorrectS(yt)×(1−logP(yt|Xbase, yt−))\n",
            "\n",
            "(5)\n",
            "4 Experiments\n",
            "4.1 Dataset\n",
            "We conduct experiments on the Mathematics problem dataset\n",
            "GSM8K [Cobbe et al. , 2021 ]. It offers mathematics prob-\n",
            "lems in natural language form, standard solution processes,\n",
            "and numerical standard answers. The training set of GSM8K\n",
            "contains 7473 entries, while the test set contains 1319 entries.\n",
            "We employed data prompting and CoT prompting to obtain\n",
            "the analysis and result (that is, the reasoning output).\n",
            "Learning from teaching: When the accuracy of the base\n",
            "model to be fine-tuned is low, we choose to use other mod-\n",
            "els with better performance to generate the analysis and re-\n",
            "sult (that is, the reasoning output). We refer to this pro-\n",
            "cess as “teaching”. Specifically, We utilized LLAMA3-8B-\n",
            "Instruct [Roziere et al. , 2023 ]to obtain the analysis and result\n",
            "of the case.\n",
            "Learning from self-study: When the model has the capac-\n",
            "ity to generate a certain percentage of correct responses, we\n",
            "proceed to let the trained model learn from its own answers\n",
            "and produce results. We refer to this process as self-study. In\n",
            "this scenario, we directly utilize the trained model to produce\n",
            "the reasoning output.\n",
            "When the reasoning output is incorrect, we leverage the\n",
            "GPT-4o [OpenAI, 2023 ]to acquire the annotations for judg-\n",
            "ment. The instruction prompt for judgment is provided in\n",
            "Appendix. A.1.\n",
            "4.2 Experimental Setup\n",
            "We conduct experiments on the LLAMA3-8B base\n",
            "model [Roziere et al. , 2023 ].\n",
            "Training Subset Setup: To investigate the learning capac-\n",
            "ity of NLFT with a small dataset, we randomly shuffle the\n",
            "data in the training set. According to the shuffled index or-\n",
            "der, we take the first 400, first 800, first 25%, first 50%, and\n",
            "100% respectively to construct training sets for experimental\n",
            "preparations.\n",
            "Hyper-parameter Settings: All experiments are carried\n",
            "out on two A800 GPUs, which is four times lower than the\n",
            "requirement demanded by reinforcement learning-based fine-\n",
            "tuning methods such as ReFT. Besides, we select the AdamW\n",
            "optimizer [Loshchilov and Hutter, 2019 ]and the cosine learn-\n",
            "ing rate scheduler. The batch size is set to be 4 and the learn-\n",
            "ing rate is set to be 5×10−5. If a small dataset is utilized,\n",
            "the model is trained for 10 epochs, while with an extensive\n",
            "dataset, training is trained for 3 epochs. For the parameters in\n",
            "NLFT, pcorrect\n",
            "0 is set to be 0.95, pincorrect\n",
            "0 is set to be 0.01,\n",
            "andr0is set to be 1.5. For the scale hyper-parameters, c1,c2,\n",
            "andc3are set to be 5, 0.3, and 0.6. Detailed hyper-parameter\n",
            "configurations are shown in Appendix B.\n",
            "Evaluation: We utilize the full dataset to assess the ac-\n",
            "curacy of natural language CoT reasoning Output from fine-\n",
            "tuned LLM. The evaluation process employs the same prompt\n",
            "templates as those used during the Training phase. In our set-\n",
            "tings, the temperature is set to 0.6 during text generation, and\n",
            "maximum generation length is 512 tokens.\n",
            "4.3 Baseline\n",
            "We compare our model NLFT with SFT [Ouyang et al. , 2022 ]\n",
            "and ReFT [Luong et al. , 2024 ]baselines. To ensure a fair\n",
            "comparison, we make sure that the hyperparameter settings\n",
            "for the SFT baseline match those of the NLFT experiments.\n",
            "The details on the hyperparameter settings is shown in Ap-\n",
            "pendix B. Besides, our study concentrates on natural lan-\n",
            "guage fine-tuning, hence we only select the corresponding\n",
            "component of ReFT, that is, the CoT-N portion. Although\n",
            "the procedural language (CoT-P) component of ReFT shows\n",
            "better performance on the GSM8K dataset for mathematical\n",
            "reasoning tasks, it significantly deviates from our experimen-\n",
            "tal setup, leading to its omission.\n",
            "4.4 Results\n",
            "Full dataset experiment: Fig. 4 compares the performance\n",
            "of NLFT and SFT baseline on GSM8K dataset. Starting\n",
            "Figure 4: Comparison of accuracy of SFT and NLFT using 25%,\n",
            "50%, 75%, and 100% of GSM8K training set, corresponding to\n",
            "1868, 3737, 5605, and 7473 samples, respectively. At proportion\n",
            "of 0 represents base model before fine-tuning.\n",
            "Figure 5: Comparison of accuracy of NLFT using minimal dataset\n",
            "samples of GSM8K as a training set, including NLFT trained with\n",
            "200 steps, 1 epoch, and 2 epochs, respectively. To better illustrate the\n",
            "increase in accuracy from the data size of 50 to 100, we additionally\n",
            "provide plots for data size of 75, under the settings of 1-epoch and\n",
            "2-epoch training.\n",
            "from the same initial base model at proportion of 01, we\n",
            "can observe that NLFT consistently achieves higher accuracy\n",
            "over SFT, with NLFT achieving an accuracy rate above 70%\n",
            "across all four percentage datasets, whereas SFT is between\n",
            "44% and 46%. NLFT outperformed SFT with an accuracy\n",
            "improvement of over 25%. Furthermore, the change in the\n",
            "proportion of the training dataset has little impact on the ac-\n",
            "curacy of both NLFT and SFT, which indicates the boundary\n",
            "effect on accuracy improvement by expanding data size will\n",
            "gradually decrease. Therefore, it is reasonable to shift our fo-\n",
            "cus to a smaller size of training dataset especially lower than\n",
            "25%, to fill the gaps of accuracy details from 0 to 25%.\n",
            "Limited-size dataset experiment: To investigate the per-\n",
            "1In our experiments, LLAMA3-8B base model failed to provide\n",
            "any reasoning for getting answer, and its generation mostly just re-\n",
            "peats the instruction prompt. The accuracy claimed in LLAMA3\n",
            "official website is achieved by LLAMA3-8B-Instruct under our re-\n",
            "production.\n",
            "Figure 6: Accuracy comparison between SFT and NLFT trained in\n",
            "minimal dataset samples with data size of 50 from 1 to 16 epochs.\n",
            "Figure 7: Comparison of GPU memory utilization between SFT,\n",
            "ReFT, and NLFT. The batch size in ReFT is set to 2. SFT and NLFT\n",
            "share the same runtime batch size of 2. NLFT has GPU memory\n",
            "usage as lightweight as SFT, which is more than 10 times lower than\n",
            "ReFT.\n",
            "formance of NLFT when using minimal dataset samples as\n",
            "a training set, we adopt data sizes ranging from 50 to 400,\n",
            "separated by 50. In Fig. 5, we conduct experiments with\n",
            "fixed training steps at 200 across different data sizes. We ob-\n",
            "serve that the first plot, utilizing shuffled 50 data entries, has\n",
            "achieved an accuracy rate of 62.93%, which is close to the\n",
            "last plot with 400 data entries.\n",
            "To reveal the intermediate states of models when LLM\n",
            "learns from different data entries, we evaluate model accu-\n",
            "racies trained after 1 and 2 epochs and plot them as dotted\n",
            "lines. In our settings, the more epochs or steps the model is\n",
            "trained, the closer it converges to the line of NLFT with 200\n",
            "steps.\n",
            "Fig. 6 compares the accuracy of NLFT over 16 continu-\n",
            "ous epochs. We observe that the accuracy in epoch 1 was\n",
            "11.30%. By epoch 2, it sharply increased to 30.8%, reaching\n",
            "the accuracy of SFT by epoch 5. Subsequently, the model\n",
            "accuracy continued to improve, reaching 60.1% by epoch 4,\n",
            "after which it remained consistently above 60%. Meanwhile,\n",
            "SFT started its rapid ascent from epoch 1 to 5, achieving the\n",
            "Figure 8: Comparison of per-epoch average time cost of NLFT, SFT,\n",
            "and ReFT. The data size of each experiment is fixed to 800. ReFT\n",
            "has a significantly higher time cost compared to NLFT and SFT.\n",
            "NLFT takes approximately 3 times longer than SFT on average.\n",
            "highest accuracy of 34.4% at epoch 8, after which it gradu-\n",
            "ally declined. These results indicate that NLFT demonstrates\n",
            "breakthrough learning potential with a limited dataset, which,\n",
            "to the best of our knowledge, is not possessed by fine-tuning\n",
            "algorithms such as SFT.\n",
            "Algorithm comparison: To validate the performance of\n",
            "different fine-tuning algorithms under identical data scales\n",
            "and training conditions, Fig. 9 conducted experiments using a\n",
            "random subset of the first 800 data points, training and testing\n",
            "the accuracy of NLFT, SFT, and ReFT after 10 epochs. The\n",
            "\n",
            "CONCISE SUMMARY in Indonesia:\n",
            "\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Write a concise summary of the following:\n",
            "breakthrough learning potential with a limited dataset, which,\n",
            "to the best of our knowledge, is not possessed by fine-tuning\n",
            "algorithms such as SFT.\n",
            "Algorithm comparison: To validate the performance of\n",
            "different fine-tuning algorithms under identical data scales\n",
            "and training conditions, Fig. 9 conducted experiments using a\n",
            "random subset of the first 800 data points, training and testing\n",
            "the accuracy of NLFT, SFT, and ReFT after 10 epochs. The\n",
            "hyperparameters for NLFT and SFT were configured consis-\n",
            "tently, while those for ReFT were set according to the origi-\n",
            "nal paper. Following [Luong et al. , 2024 ], ReFT trains on the\n",
            "basis of SFT warm-up with 2 epochs. In the first test, we ob-\n",
            "served that ReFT accuracy persistently declined over the first\n",
            "4 epochs, and dropped to zero by epoch 52. To ensure the\n",
            "experimental result is correct, we conducted a second experi-\n",
            "ment with ReFT, maintaining a data scale of 800 instances,\n",
            "and observed a similar decline to zero accuracy by the 6\n",
            "epoch. Upon reviewing the output, we discovered that the\n",
            "model regraded to repeating the instruction. This degradation\n",
            "is attributed to the instability of ReFT (and other reinforce-\n",
            "ment learning-based fine-tuning algorithms), which have sig-\n",
            "nificant requirements for data quantity. While ReFT can learn\n",
            "effectively with the full dataset, it often reverts to a state of\n",
            "non-learning when data quantity is insufficient. Under 800\n",
            "data samples, SFT is capable of learning effectively, reaching\n",
            "an accuracy of 39.88% at epoch 5, after which the accuracy\n",
            "had a decline of over 10%. In contrast, NLFT achieved an\n",
            "accuracy of 71.65%, with no significant drop in accuracy fol-\n",
            "lowing the epoch of peak performance. This demonstrates\n",
            "the universality of NLFT with respect to data quantity and\n",
            "2We reproduced ReFT, and experiments show that under full\n",
            "dataset of GSM8K, ReFT can improve accuracy on top of SFT-\n",
            "warmup models, just as in the original paper. However, here we fo-\n",
            "cus on experiments of minimal data fine-tuning, and in order to make\n",
            "consistency with NLFT and SFT experiments, we adopted training\n",
            "results with 800 data points instead of full dataset.\n",
            "Figure 9: Comparison of accuracy of NLFT, SFT, ReFT with data size of 800. Both experiments of ReFT are pre-warmed using SFT, and the\n",
            "initial accuracy is shown in epoch 0.\n",
            "the efficiency of its training outcomes.\n",
            "Time Cost Analysis: Under 800 data samples, we con-\n",
            "ducted a comparative analysis of the training time required\n",
            "for NLFT, SFT, and ReFT across 10 epochs. Since ReFT al-\n",
            "gorithm cannot be executed in two-GPU configuration, we\n",
            "recorded time consumption of each algorithm under an eight-\n",
            "GPU configuration for fair comparison. As shown in Fig. 8,\n",
            "ReFT required an average of 30 minutes per epoch, whereas\n",
            "NLFT, due to the use of more GPUs, saw a significant reduc-\n",
            "tion in training time compared to 26.1 minutes the two-GPU\n",
            "setup, averaging 6.5 minutes.\n",
            "It is worth noticing that, the time consumption ratio be-\n",
            "tween NLFT and SFT is around 3. NLFT involves three times\n",
            "the forward inference processes compared to SFT, hence its\n",
            "time complexity constant is at least 3. Despite the increased\n",
            "constant term, NLFT still qualifies as a lightweight fine-\n",
            "tuning algorithm with linear time complexity.\n",
            "Memory Use Analysis: Fig. 7 illustrates the runtime GPU\n",
            "memory usage of each fine-tuning algorithm. With a two-\n",
            "GPU configuration, SFT averages a total memory usage of\n",
            "44.55 GB, while NLFT averages 46.87 GB. NLFT’s memory\n",
            "usage is only 5.2% higher than SFT’s, which still falls within\n",
            "the category of lightweight fine-tuning algorithms. In con-\n",
            "trast, ReFT requires an average of 599.57 GB of total mem-\n",
            "ory, which is not in the same order of magnitude as NLFT.\n",
            "Regarding hardware configuration and memory usage, NLFT\n",
            "not only matches SFT’s requirements but also significantly\n",
            "outperforms reinforcement learning-based fine-tuning algo-\n",
            "rithms like ReFT, offering a unique advantage in terms of\n",
            "hardware resource demands.\n",
            "5 Analysis\n",
            "5.1 Stability Analysis\n",
            "Compared with SFT and ReFT, the NLFT shows a robust ca-\n",
            "pacity to mitigate overfitting in minimal data scenarios. Fig. 6\n",
            "shows the performance of SFT and NLFT with a minimal data\n",
            "dataset of 50 samples across 1-16 epochs. It can be observed\n",
            "that SFT exhibits pronounced overfitting, while NLFT main-\n",
            "tains a consistent level of accuracy. Fig. 9 shows the perfor-\n",
            "mance of ReFT, SFT, and NLFT with a dataset of 800 sam-ples. It can be observed that SFT exhibits slow improvement\n",
            "in the initial epochs and reaches near-optimal performance\n",
            "after 5 epochs. However, as the number of epochs increases,\n",
            "the risk of overfitting grows, leading to a significant over-\n",
            "fitting and a sharp decline in accuracy. In contrast, NLFT\n",
            "exhibits a stable accuracy of approximately 70%, reflecting\n",
            "its robust stability. We suppose this is because the algorithm\n",
            "shows more attention to the saliency token, thereby focusing\n",
            "on the most critical problem-solving pathway and improving\n",
            "training stability. This approach is similar to causal-inspired\n",
            "stable learning in computer vision [Zhang et al. , 2021 ], which\n",
            "effectively filters out irrelevant features and uses only truly\n",
            "relevant ones for prediction, resulting in more stable perfor-\n",
            "mance in wild, non-stationary environments.\n",
            "5.2 Algorithm Complexity Analysis\n",
            "The time complexity and space complexity of our algorithm\n",
            "are both O(n), which means that as the input size ngrows,\n",
            "the required time and space resources grow linearly. This\n",
            "linear growth indicates that our algorithm is efficient and re-\n",
            "source consumption is controllable when dealing with large-\n",
            "scale data. In contrast, the ReFT algorithm uses the PPO al-\n",
            "gorithm [Schulman et al. , 2017 ]for optimization, whose time\n",
            "complexity is O(TC+NBP ), which is proportional to the\n",
            "product of the number of samples, the neural network’s for-\n",
            "ward and backward propagation complexity, and the number\n",
            "of update iterations [Luong et al. , 2024 ]. Due to its higher\n",
            "time and space complexity, ReFT may suffer from decreasing\n",
            "efficiency and increasing resource consumption when pro-\n",
            "cessing large data. Therefore, in applications involving large-\n",
            "scale data processing, our algorithm demonstrates a signifi-\n",
            "cant performance advantage over the ReFT.\n",
            "6 Discussion\n",
            "6.1 Learning Efficiency from Incorrect Samples\n",
            "Our experiments indicate that increasing the proportion of\n",
            "incorrect samples in the training data leads to a decrease in\n",
            "learning efficiency. To fine-grained investigate the contribu-\n",
            "tion of incorrect samples to model fine-tuning, we attempt to\n",
            "Figure 10: Token-level selection comparison between CUT and\n",
            "NLFT on a single instance from the GSM8K dataset. Green tokens\n",
            "are marked by CUT, blue tokens by NLFT with ratio values, red to-\n",
            "kens represent incorrect selections, and red circles indicate correctly\n",
            "marked tokens.\n",
            "visualize saliency tokens at the token level to reflect the in-\n",
            "termediate process of LLM training. In Fig. 10, we compare\n",
            "NLFT with another token-level LLM fine-tuning algorithm,\n",
            "CUT [Xuet al. , 2023 ], and mark the recognized incorrect\n",
            "token of NLFT and CUT. The results demonstrate that com-\n",
            "pared to the CUT algorithm, NLFT can more accurately iden-\n",
            "tify incorrect tokens in answers. Additionally, we implement\n",
            "a filtering strategy for entirely incorrect samples. When the\n",
            "proportion of erroneous tokens exceeds a certain threshold,\n",
            "NLFT will exclude that training data.\n",
            "However, after such processing, the efficiency of learning\n",
            "from incorrect samples still remains significantly lower com-\n",
            "pared to learning from correct samples. We analyze that, on\n",
            "one hand, the value of acquiring new knowledge may out-\n",
            "weigh the restrain of incorrect tokens; on the other hand, af-\n",
            "ter multiple rounds of training, the model may exhibit a phe-\n",
            "nomenon of ”logical coherence”, it avoids marking incorrect\n",
            "tokens, thereby reducing the MLE loss. Regarding how to en-hance the learning efficiency from incorrect samples, we will\n",
            "continue to explore this in our subsequent research.\n",
            "6.2 Model Generalization\n",
            "In our previous research, we applied the simplified version\n",
            "of the NLFT to human-computer collaborative tasks and\n",
            "achieved significant performance improvement. Theoreti-\n",
            "cally, NLFT is applicable to scenarios where outputs can be\n",
            "generated through CoT and labeled data is available, such as\n",
            "coding, medical diagnosis, natural language inference, and\n",
            "complex question-answering systems. By comparing the gen-\n",
            "erated output with the labels, it is possible to annotate the\n",
            "saliency tokens, thereby applying NLFT for token-level fine-\n",
            "tuning. In our future work, we will explore the application of\n",
            "NLFT to broader fields and refine the NLFT algorithm based\n",
            "on the characteristics of each task.\n",
            "7 Conclusion\n",
            "In this paper, we propose a novel natural language minimal\n",
            "data fine-tuning algorithm NLFT. The algorithm compares\n",
            "the conditional probabilities of various natural language to-\n",
            "kens under different prompts, utilizing natural language as a\n",
            "supervisory signal to identify saliency tokens and assign them\n",
            "scaling values. Experimental results demonstrate that our al-\n",
            "gorithm, compared to previous ones, has lower time com-\n",
            "plexity and better performance. Under the GSM8K dataset\n",
            "evaluation, only random 50 training data allows NLFT to\n",
            "achieve over 60% accuracy, and performance of NLFT is\n",
            "stably increased by 25% compared to SFT. In contrast to\n",
            "RL-based fine-tuning algorithm like ReFT, NLFT saves huge\n",
            "time and space complexity, enabling broader imagination for\n",
            "lightweight fine-tuning and applications.\n",
            "References\n",
            "[Chuet al. , 2023 ]Zheng Chu, Jingchang Chen, Qianglong\n",
            "Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng,\n",
            "Ming Liu, Bing Qin, and Ting Liu. A survey of chain of\n",
            "thought reasoning: Advances, frontiers and future. arXiv\n",
            "preprint arXiv:2309.15402 , 2023.\n",
            "[Cobbe et al. , 2021 ]Karl Cobbe, Vineet Kosaraju, Moham-\n",
            "mad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\n",
            "Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro\n",
            "\n",
            "CONCISE SUMMARY in Indonesia:\n",
            "\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Write a concise summary of the following:\n",
            "lightweight fine-tuning and applications.\n",
            "References\n",
            "[Chuet al. , 2023 ]Zheng Chu, Jingchang Chen, Qianglong\n",
            "Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng,\n",
            "Ming Liu, Bing Qin, and Ting Liu. A survey of chain of\n",
            "thought reasoning: Advances, frontiers and future. arXiv\n",
            "preprint arXiv:2309.15402 , 2023.\n",
            "[Cobbe et al. , 2021 ]Karl Cobbe, Vineet Kosaraju, Moham-\n",
            "mad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\n",
            "Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro\n",
            "Nakano, Christopher Hesse, and John Schulman. Training\n",
            "Verifiers to Solve Math Word Problems, November 2021.\n",
            "[Feng et al. , 2024a ]Guhao Feng, Bohang Zhang, Yuntian\n",
            "Gu, Haotian Ye, Di He, and Liwei Wang. Towards re-\n",
            "vealing the mystery behind chain of thought: a theoretical\n",
            "perspective. Advances in Neural Information Processing\n",
            "Systems , 36, 2024.\n",
            "[Feng et al. , 2024b ]Xidong Feng, Ziyu Wan, Haotian Fu,\n",
            "Bo Liu, Mengyue Yang, Girish A. Koushik, Zhiyuan Hu,\n",
            "Ying Wen, and Jun Wang. Natural language reinforcement\n",
            "learning, 2024.\n",
            "[Huet al. , 2021 ]Edward J. Hu, Yelong Shen, Phillip Wallis,\n",
            "Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n",
            "and Weizhu Chen. Lora: Low-rank adaptation of large\n",
            "language models, 2021.\n",
            "[Liet al. , 2019 ]Haidong Li, Jiongcheng Li, Xiaoming\n",
            "Guan, Binghao Liang, Yuting Lai, and Xinglong Luo. Re-\n",
            "search on overfitting of deep learning. In 2019 15th In-\n",
            "ternational Conference on Computational Intelligence and\n",
            "Security (CIS) , pages 78–81, 2019.\n",
            "[Loshchilov and Hutter, 2019 ]Ilya Loshchilov and Frank\n",
            "Hutter. Decoupled Weight Decay Regularization, January\n",
            "2019.\n",
            "[Luong et al. , 2024 ]Trung Quoc Luong, Xinbo Zhang,\n",
            "Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft:\n",
            "Reasoning with reinforced fine-tuning. arXiv preprint\n",
            "arXiv:2401.08967 , 2024.\n",
            "[OpenAI, 2023 ]R OpenAI. Gpt-4 technical report. arxiv\n",
            "2303.08774. View in Article , 2(5), 2023.\n",
            "[OpenAI, 2024 ]OpenAI. Reinforcement learning from hu-\n",
            "man feedback research program, 2024. Accessed: 2024-\n",
            "12-23.\n",
            "[Ouyang et al. , 2022 ]Long Ouyang, Jeffrey Wu, Xu Jiang,\n",
            "Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\n",
            "Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex\n",
            "Ray, et al. Training language models to follow instruc-\n",
            "tions with human feedback. Advances in neural informa-\n",
            "tion processing systems , 35:27730–27744, 2022.\n",
            "[Rafailov et al. , 2024a ]Rafael Rafailov, Joey Hejna, Ryan\n",
            "Park, and Chelsea Finn. From rtoq∗: Your lan-\n",
            "guage model is secretly a q-function. arXiv preprint\n",
            "arXiv:2404.12358 , 2024.\n",
            "[Rafailov et al. , 2024b ]Rafael Rafailov, Archit Sharma, Eric\n",
            "Mitchell, Christopher D Manning, Stefano Ermon, and\n",
            "Chelsea Finn. Direct preference optimization: Your lan-\n",
            "guage model is secretly a reward model. Advances in Neu-\n",
            "ral Information Processing Systems , 36, 2024.\n",
            "[Roziere et al. , 2023 ]Baptiste Roziere, Jonas Gehring,\n",
            "Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen\n",
            "Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez,\n",
            "et al. Code llama: Open foundation models for code.\n",
            "arXiv preprint arXiv:2308.12950 , 2023.\n",
            "[Schulman et al. , 2017 ]John Schulman, Filip Wolski, Pra-\n",
            "fulla Dhariwal, Alec Radford, and Oleg Klimov. Prox-\n",
            "imal policy optimization algorithms. arXiv preprint\n",
            "arXiv:1707.06347 , 2017.\n",
            "[von Werra et al. , 2020 ]Leandro von Werra, Younes\n",
            "Belkada, Lewis Tunstall, Edward Beeching, Tristan\n",
            "Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul,\n",
            "and Quentin Gallou ´edec. Trl: Transformer reinforcement\n",
            "learning. https://github.com/huggingface/trl, 2020.\n",
            "[Wang et al. , 2022 ]Xuezhi Wang, Jason Wei, Dale Schu-\n",
            "urmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\n",
            "Chowdhery, and Denny Zhou. Self-consistency improves\n",
            "chain of thought reasoning in language models. arXiv\n",
            "preprint arXiv:2203.11171 , 2022.\n",
            "[Weiet al. , 2022 ]Jason Wei, Xuezhi Wang, Dale Schuur-\n",
            "mans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\n",
            "Denny Zhou, et al. Chain-of-thought prompting elicitsreasoning in large language models. Advances in neural\n",
            "information processing systems , 35:24824–24837, 2022.\n",
            "[Xuet al. , 2023 ]Weiwen Xu, Deng Cai, Zhisong Zhang,\n",
            "Wai Lam, and Shuming Shi. Reasons to reject? align-\n",
            "ing language models with judgments. arXiv preprint\n",
            "arXiv:2312.14591 , 2023.\n",
            "[Zhang et al. , 2021 ]Xingxuan Zhang, Peng Cui, Renzhe Xu,\n",
            "Linjun Zhou, Yue He, and Zheyan Shen. Deep stable\n",
            "learning for out-of-distribution generalization. In Proceed-\n",
            "ings of the IEEE/CVF Conference on Computer Vision and\n",
            "Pattern Recognition , pages 5372–5382, 2021.\n",
            "[Zhong et al. , 2024 ]Han Zhong, Guhao Feng, Wei Xiong,\n",
            "Li Zhao, Di He, Jiang Bian, and Liwei Wang. Dpo\n",
            "meets ppo: Reinforced token optimization for rlhf. arXiv\n",
            "preprint arXiv:2404.18922 , 2024.\n",
            "A Prompt Strategies\n",
            "A.1 Prompt Strategy of Judgment on the GSM8K\n",
            "Dataset\n",
            "In this section, we present a prompt engineering strategy for\n",
            "generating the judgment of incorrect samples.\n",
            "Instruction\n",
            "Suppose you are a math expert and you are presented\n",
            "with a math problem, a student’s response, and the\n",
            "correct answer.\n",
            "Please first check whether the student’s response is\n",
            "correct. Including checking the solution process and\n",
            "whether the answer is correct.\n",
            "If the student’s response is correct, please directly\n",
            "output: ### The response is correct. ###\n",
            "If the response is wrong, please analyze why the so-\n",
            "lution is wrong.\n",
            "A.2 Prompt Strategy of Math Reasoning on the\n",
            "GSM8K Dataset\n",
            "In this section we present a prompt engineering strategy for\n",
            "testing model performance.\n",
            "Instruction\n",
            "Suppose you are a math expert. The following de-\n",
            "scribes a math problem. Please read it carefully and\n",
            "solve it STEP BY STEP!!!, and give the correct an-\n",
            "swer.\n",
            "Please ensure that your output strictly follows the fol-\n",
            "lowing format requirements:\n",
            "{Your analysis }\\n#### {The answer number }\n",
            "Your analysis should be very detailed. And make sure\n",
            "the string ”####” only appears once following the an-\n",
            "swer number in the end.\n",
            "For example:\n",
            "<Output Example >\n",
            "Natalia sold 48 /2 =<<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72 >>72 clips alto-\n",
            "gether in April and May.\n",
            "#### 72\n",
            "</Output Example >\n",
            "<Output Example >\n",
            "The number of truck stamps is 11 + 9 =\n",
            "<<11+9=20 >>20.\n",
            "The number of rose stamps is 20 - 13 = <<20-\n",
            "13=7>>7.\n",
            "Bella bought 11 + 20 + 7 = <<11+20+7=38 >>38\n",
            "stamps in all.\n",
            "#### 38\n",
            "</Output Example >\n",
            "<Output Example >\n",
            "Lisa earned $60 * 1 /2 = $<<60*1/2=30>>30.\n",
            "Tommy earned $30 * 1 /2 = $<<30*1/2=15>>15.\n",
            "Lisa earned $30 - $15 = $ <<30-15=15 >>15 more\n",
            "than Tommy.\n",
            "#### 15\n",
            "</Output Example >\n",
            "<Output Example >\n",
            "He needs to save up $400 because 4 x 100 =\n",
            "<<4*100=400 >>400\n",
            "He has 8 months to earn this money because 12 - 4 =\n",
            "<<12-4=8 >>8\n",
            "He needs to earn $50 a month because 400 /8 =\n",
            "<<400/8=50>>50\n",
            "He needs to do 5 tasks a month because 50 /10 =\n",
            "<<50/10=5>>5\n",
            "#### 5\n",
            "</Output Example >\n",
            "<Output Example >\n",
            "15 coins collected in hour one\n",
            "35 coins collected in hour two\n",
            "35 coins collected in hour three\n",
            "50 coins collected in hour four\n",
            "Before giving her coworker some coins there were\n",
            "15+35+35+50= <<15+35+35+50=135 >>135 coins\n",
            "The number of coins after giving 15 to her coworker\n",
            "is 135-15= <<135-15=120 >>120\n",
            "#### 120\n",
            "</Output Example >B Detailed Hyperparameter Settings\n",
            "NLFT: NLFT is trained using LoRA [Huet al. , 2021 ], where\n",
            "the parameters r,αand dropout are set to 16, 16, and 0.05,\n",
            "respectively. The learning rate is set to 5×10−5. For most\n",
            "NLFT experiments, the maximum number of epochs is set to\n",
            "10, unless the setting in Fig. 9 specifies 30.\n",
            "SFT: Our SFT implementation employs SFTTrainer in\n",
            "trl[von Werra et al. , 2020 ]. To ensure that the SFT code\n",
            "configuration is largely consistent with NLFT configuration,\n",
            "we have essentially adopted most of the parameter settings of\n",
            "NLFT.\n",
            "ReFT: Following [Luong et al. , 2024 ], before ReFT algo-\n",
            "rithm, we perform SFT warmup for 2 epochs with the learn-\n",
            "ing rate of 1×10−5on GSM8K dataset. When performing\n",
            "SFT warmup, the batch size is set to 48, and the maximum\n",
            "input length is set to 512. After warmup phase is finished, we\n",
            "perform ReFT algorithm. The maximum input length is set\n",
            "to 300, and the maximum length of model generation is set to\n",
            "700. The batch size is set to 16 to avoid crash during training.\n",
            "The number of updates per RL step is set to 2. The learning\n",
            "rate is set to 3×10−7.\n",
            "\n",
            "CONCISE SUMMARY in Indonesia:\n",
            "\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Write a summary of the following text delimited by tripple backquotes.\n",
            "Return your response as detailed summary which covers the main points of the text and key facts and figures.\n",
            "\n",
            "``` Kami menyusun algoritma pemantauan bahasa alam yang menganalisis probabilitas conditional token setiap token di berbagai kondisi prompt. Algoritma tersebut menggabungkan analisis langkah demi langkah dari pemantauan lembar jawaban, sehingga mempercepat proses pemodelan pemantauan model yang lebih efisien.\n",
            "\n",
            "Penelitian ini memberikan dua kontribusi:\n",
            "1. Paparnya menyajikan Natural Language Fine-Tuning (NLFT), sebuah algoritma pemodelan bertokentan. NLFT merupakan alternatif terhadap metode pemodelan yang menggunakan skor skala yang diperkenalkan oleh metode respon level sebelumnya.\n",
            "2. Paparnya memproyeksikan algoritma minimal data yang tidak memerlukan tahap warm-up, yang diperlukan oleh proses ReFT. Hal ini meningkatkan akurasi hasil pemodelan yang sangat signifikan dibandingkan dengan SFT dengan jumlah data yang singkat. Selain itu, algoritma kami mempunyai keunggulan dalam penggunaan memori GPU dan waktu pelatihan, sehingga lebih efisien dibandingkan dengan metode lainnya (seperti ReFT).\n",
            "\n",
            " Metode pelatihannya melibatkan pemilihan input X yang terdiri dari pertanyaan, jawaban standar, dan pendafaran. Setelah mencocokkan persentase setiap token, mereka menempatkan kata kunci saliency pada berdasarkan seberapa signifikannya dalam setiap input X. Kata kunci sub-saliency lalu diidentifikasi dengan mengklustering semantik kembali, yang menjadikannya seluruh k\n",
            "\n",
            " Model pembelajaran yang terbahar diberikan dengan tampilan dan performa model pembelajaran yang terbahar, yaitu deep learning (DL) dengan metode fine-tuning. Kami mencoba menyampaikan informasi tentang bagaimana setiap metode fine-tuning berbeda dalam konteks khusus ini dengan membandingkan antara Natural Language Processing (NLP) model yang telah diberi pelatihan lewat pembelajaran terbahar dari dua metode, SFT (Simple Fine-tuning) dan NLFT (Neural Languague Model Fine-tuning).\n",
            "\n",
            "SFT adalah metode pembelajaran yang sangat sederhana yang menggunakan model pembelajaran abstrak siap pakai sebagai titik awal untuk melakukan pembelajaran terbahar. Di sisi lain, NLFT menerapkan metode pembelajaran yang lebih canggih yang diperuntukan untuk memodifikasi model pembelajaran abstrak siap pakai dengan mengintegrasikan k\n",
            "\n",
            " Algoritma Peminimalisasi Data Natural Language yang Baru\n",
            "\n",
            "Kami menyajikan algoritma peminimalisasi data natural language baru (NLFT) unti kewajiban menganalisis dan membandingkan kondisi probabilitas token-token bahasa yang berbeda di bawah prompt. Dengan menggunakan alur bahasa sebagai tanda supervisi, algoritma ini dapat mengidentifikasi saliency tokens dan memberikan nilai skala untuk mereka. Pengujian yang dilakukan menunjukkan bahwa algoritma kami memiliki kompleksitas waktu yang lebih rendah daripada algoritma sebelumnya dan memiliki performansi yang lebih baik. Di dataset GSM8K, hanya 50 data training internal akan memungkinkan NLFT untuk mencapai keterampilan yang lebih dari 60%, sedangkan prestasinya stabil meningkat sebesar 25% dibandingkan dengan SFT. Pada sisi lain, algoritma NLFT menyimpan biaya waktu dan ruang yang signifikan untuk aplikasi RL seperti ReFT.\n",
            "\n",
            "Referensi:\n",
            "[Chuet al. , 2023 ]Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, dan Ting Liu. Sejarah dan pengembangan chain of thought reasoning: Fitur terbaru, kedekatan, dan kebangkitan. Preprint arXiv preprint arXiv:2309.15402 , 2023.\n",
            "[Cobbe et al. , 2021 ]Karl Cobbe, Vineet Kosaraju, Mohamad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, dan Reiichiro Kakiuchi. A survey of chain of thought reasoning: Advances, frontiers and future. arXiv preprint arXiv:2109.13865 , 2021.\n",
            "\n",
            " Kami menggabungkan tiga metode pembelajaran untuk membangun model bahasa yaitu Language Model Fine-tuning (NLFT), Structure-aware Fine-tuning (SFT), dan Retrieval-augmented Generation Fine-tuning (ReFT).\n",
            "\n",
            "Pada tahap NLFT, kami menggunakan LoRA dengan parameternya seperti yang disediakan oleh [Huet al. , 2021]. Kami mengatur learning rate sebesar 5×10^-5 dan mengatur jumlah epoch hingga 10, tetapi jika setting tersebut di Figur 9, kita mengatur jumlah epoch menjadi 30.\n",
            "\n",
            "SFT yang kami gunakan berasal dari trl[von Werra et al., 2020]. Kami mencoba untuk memadukan konfigurasi SFT dengan NLFT sehingga semua parameternya setara.\n",
            "\n",
            "Pada tahap ReFT, kami melakukan perwarmup yang bertujuan SFT selama 2 epoch dengan learning rate 1×10^-5 di dataset GSM8K. Saat melakukan perwarmup, kita mengatur batch size menjadi 48 dan panjang maksimal input menjadi 512. Setelah selesai dengan tahap perwarmup, kami melakukan tahap ReFT. Kita mengatur panjang maksimal input sebesar 300 dan panjang maksimum generasi model menjadi 700. Kami juga mengatur batch size menjadi 16 untuk mencegah penimpaan saat melakukan pembelajaran. Setiap perunggu pada langkah RL, kita mengatur jumlah update sebesar 2. Kami juga mengatur learning rate sebesar 3×10^-7.\n",
            "\n",
            "Untuk setiap metode pembelajaran, kami meyatkan detail konfigurasi yang spesifik di bagian ini.```\n",
            "\n",
            "DETAILED SUMMARY in Indonesia:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Content length: 764 chars, 272 tokens.\n",
            "Summary:\n",
            " Dokumen ini mengajukan algoritma pemantauan bahasa alam yang menyusun analisis probabilitas conditional token setiap token di berbagai kondisi prompt. Algoritma ini terdiri dari tiga metode pembelajaran: Language Model Fine-tuning (NLFT), Structure-aware Fine-tuning (SFT), dan Retrieval-augmented Generation Fine-tuning (ReFT). Tujuannya adalah memperbaiki keterampilan model bahasa. Kegiatan ini menggunakan Learning Rate sebesar 5x10^-5 dan mengatur jumlah epoch hingga 10, tetapi jika setting tersebut di Figur 9, jumlah epochnya diatur menjadi 30. Algoritma ini memiliki biaya waktu dan ruang yang signifikan untuk aplikasi RL seperti ReFT. Pengembangan algoritma ini terinspirasi dari [Chu et al., 2023], [Cobbe et al., 2021], dan [von Werra et al., 2020].\n",
            "\n",
            "\n",
            "\n",
            "CPU times: total: 547 ms\n",
            "Wall time: 16min 59s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "content = load_content()\n",
        "content_tokens = llm.get_num_tokens(content)\n",
        "print(f\"Content length: {len(content)} chars, {content_tokens} tokens.\")\n",
        "print(\"Content sample:\\n\" + content[:200] + \"\\n\\n\")\n",
        "\n",
        "# Keep part of context window for models output.\n",
        "base_threshold = 0.75*MODEL_CONTEXT_WINDOW\n",
        "\n",
        "if (content_tokens < base_threshold):\n",
        "    print(\"Using summarizer: base\")\n",
        "    summary = summarize_base(llm, content)\n",
        "else:\n",
        "    print(\"Using summarizer: map-reduce\")\n",
        "    summary = summarize_map_reduce(llm, content)\n",
        "\n",
        "print(f\"Content length: {len(summary)} chars, {llm.get_num_tokens(summary)} tokens.\")\n",
        "print(\"Summary:\\n\" + summary + \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWXfaqf2sfZj"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnnhWLCNsfZj",
        "outputId": "ac6e9863-7cc9-4d4a-b477-e5609ee27e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File PDF 'output_detail12.pdf' berhasil dibuat.\n"
          ]
        }
      ],
      "source": [
        " from fpdf import FPDF\n",
        "\n",
        "\n",
        "def generate_pdf(filename, summary):\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.multi_cell(0, 10, summary)\n",
        "    pdf.output(filename)\n",
        "    print(f\"File PDF '{filename}' berhasil dibuat.\")\n",
        "\n",
        "# Contoh penggunaan\n",
        "summary_text = summary\n",
        "generate_pdf(\"output_detail12.pdf\", summary_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k83Nj6SsfZj"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeLETcsfsfZk"
      },
      "source": [
        "##"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}